# Log-Structured Non-Volatile Main Memory

新出现的非易失性主存（NVMM）通过在主存中存储持久性数据来解锁应用程序的性能潜力。这类应用程序需要一个轻量级的持久性事务性内存（PTM）系统，而不是一个重量级的文件系统或数据库，才能快速访问数据。在PTM系统中，内存的容量和带宽在决定性能和效率方面起着关键作用。现有的ptm的内存管理机制会产生高内存碎片、高写入流量和大量的持久性障碍，因为数据首先被写入日志，然后又被写入主数据存储。

在本文中，**我们提出了一个日志结构的NVMM系统，它不仅以紧凑的方式维护NVMM，而且还减少了执行事务所需的写流量和持久化障碍的数量。所有数据分配和修改都附加到日志中，成为数据的位置**。此外，我们通过设计一种基于**树的地址转换机制来解决日志结构化内存管理**的一个独特挑战，该机制中的访问粒度非常灵活，并且不同于分配粒度。研究结果表明，与传统的PTM系统相比，新系统的事务吞吐量提高了89.9%，写流量降低了82.8%。

## Introduction

新出现的字节寻址非易失性主存（NVMM），例如3D XPoint [23]、PCM [43,27]、STTRAM [3,25]和ReRAM [2]，使持久数据存储在主存中。这导致了一种应用程序通过CPU加载/存储指令[50,10,49,37,44,18,41,55]直接访问持久数据的架构。这种体系结构降低了延迟，不仅是因为NVMM比ssd具有显著更高的性能，而且还因为系统软件被从持久性数据访问[36,13,9,55]的关键路径中删除。使用NVMM的应用程序通常使用轻量级的持久性事务性内存（PTM）系统[50,10,22,56,34,9,18,24]，而不是传统的文件系统或数据库，以便能够快速访问NVMM数据。

现有PTM系统用于管理NVMM的dram式内存管理会导致大量碎片，这可能导致超过50%空间[46]的损耗。此外，PTM系统使用的现有事务机制会导致过多的写操作，因为它们要求所有新数据被写入两次——一次到日志，一次到主数据区域，称为数据的主空间。冗余写入不仅增加内存带宽使用，而且更快地磨损NVMM设备。此外，这些写需要使用昂贵的同步障碍持久化，从而增加事务的延迟。

在本文中，我们提出了一种新的针对NVMM系统的日志结构化内存管理模型。该模型消除了家庭空间和单独对数区域的二分法。**我们通过仅以日志的形式组织整个NVMM来统一家庭空间和日志区域，日志也作为家庭空间**。我们的设计有效地减少了碎片化，结合了磨损水平，并优化了写入流量和持续存在的障碍。碎片是最小的，因为内存分配成为日志结尾的即时附加，并且释放的区域可以移动和合并[45,46]，以进一步减少碎片。此外，由于不需要将数据分别写入传统的家庭空间和日志，因此减少了NVMM的带宽消耗、写入磨损和持久化障碍的数量。

使用我们的系统的应用程序以与传统系统相同的方式查看NVMM，但是使用了运行时地址映射机制来将应用程序地址转换为日志偏移量。我们将应用程序的NVMM视图称为虚拟家庭空间。这样的地址映射完全缓存在DRAM中，并且可以在崩溃后从日志中恢复。

这项工作的另一个关键贡献是在我们的系统中设计和实现了一个**实用的树形数据结构**。虽然已经在不同的领域中探索了日志结构化方法，如文件系统[45,52]、数据库[48,46,4]和对象存储[31,46]，但日志结构化NVMM面临着地址映射开销的独特挑战。与现有的对数结构化系统不同，**我们需要提供一个平坦的地址空间，其中分配粒度与访问粒度不相同**。

...

一个简单的树数据结构需要每次内存访问的O（log n）操作，当n很大时，这可能是禁止的。此外，树需要昂贵的平衡操作来实现这样的时间复杂度。我们为对数结构的NVMM设计了一个树状结构的密钥优化，以减少地址转换开销： (1)**双层映射**。首先将整个主空间划分为静态的固定长度的分区，这样数据就可以在O (1)个时间内路由到这样的一个分区（或更多的分区）。这样，分区-局部树中的平均节点数量比覆盖整个地址空间的巨大树要小得多。(2)**跳列表树**。我们对第二层树使用跳跃列表[42]。其主要好处是，它们在插入时间进行概率平衡，以避免重新平衡操作，这是昂贵的，并在很大程度上损害了并行性。(3)**组更新**。如果连续写入目标连续地址，我们将它们合并并只更新树一次。(4)树形节点高速缓存。我们观察到，内存访问具有局部性，因此缓存最近访问过的树节点可以避免许多从根节点开始的完整的树查找路径。

我们还提出了一些机制来控制压缩所需的日志清理开销，并加快恢复过程。NVMM日志可以在恢复时并行处理，这有助于在3.0秒内重建10 GB NVMM的地址映射。

总的来说，我们做出了以下贡献：一种新的对数结构化设计，以消除PTMs的数据和事务性日志之间的二分法。我们确定了现有的对数结构系统和nvmm所需的类型之间的关键区别，其中访问粒度与分配粒度不完全相同。•一种新的基于树的地址映射机制。据我们所知，我们是第一个演示在对数结构的NVMM系统中使用这种经过良好优化的树状结构的实用性的人。•通过修改TinySTM [16]来实现上述思想。在各种工作负载下，当NVMM的使用超过90%并且发生日志清理开销时，日志结构化NVMM平均比传统PTM高出55.3%的吞吐量，减少72.2%的写入磨损。

## 背景

**NVMM空间的碎片化**。在一个传统的内存分配器[17,15]中有两个碎片化的来源。首先是内部分裂。以英特尔的NVML [22]为例。它将任何NVMM分配大小调整为64 B。如果请求65 B的NVMM，NVML应有效分配128 B，包括63 B内部碎片。其次是外部碎片化。假设一个64-B块被释放，但周围的块正在使用，那么它不能服务于超过64 B的任何请求。如果分配大小改变[38]，外部碎片会很严重。实验[46]已经证明，碎片化可以占据所管理下的所有内存的50%以上。这个问题对NVMM来说更为重要，因为它可以长期保存数据，甚至在重新启动期间。

**过多的NVMM写入流量和障碍**。NVMM在带宽和耐久性[28]方面有限制（与DRAM的10^15个周期相比，有10^4个−10^9 P/E周期）。但是，为了保持崩溃一致性，所有NVMM写必须首先由PTM记录在单独的位置。与天真写入相比，这种日志记录需要冗余的NVMM写入流量和额外的磨损。

图1显示了日志结构化方法如何减少具有代表性事务的写流量和刷新数。通过伪函数映射地址，该区域内的所有地址都被映射到日志中的一个新位置。这种映射只涉及DRAM写写，在NVMM上没有磨损。这种方法节省了额外的NVMM写入操作和昂贵的CPU刷新/持久化障碍。

**日志结构化NVMM中的一个独特挑战**。基于树的地址映射的挑战是日志结构NVMM唯一的挑战。在现有的日志结构系统中没有看到。这些系统以定义良好的元素的形式管理数据，例如文件系统[45,52]中的块、数据库[48,4]中的元组或键值存储[31,46]中的对象，其中分配粒度与访问粒度相同。这种定义良好的访问粒度有助于进行高性能的设计。例如，可以使用内存中的散列表将元素映射到它们在日志中的位置，这提供了O (1)查找。此外，在存在缓慢的搜索路径的情况下（例如，对数结构的合并树[48]），可以应用一个bloom过滤器来改善映射/索引性能。

不幸的是，NVMM系统缺少这样的便利性。在裸内存中没有数据元素或id的概念。在使用平坦地址空间的系统中，很难定义一个，其中访问可以针对任何长度的偏移量。抑制块/对象粒度访问缺乏灵活性，并导致成本较高的[51,16]。简单地设置一个固定的小块大小（例如，几十个字节）也是不可行的，因为用于维护这些块的元数据可能是非常大的[30,19]。此外，NVMM比ssd快几个数量级，所以地址映射开销虽然传统上可以忽略不计，但现在却脱颖而出。因此，我们设计了一个更灵活但高性能的方案，它根据执行的存储指令根据需要分割地址空间，而不是静态或在数据分配时间定义粒度。

## 设计

### overview

LSNVMM的高级体系结构如图2所示。从下而上，LSNVMM通过文件系统使用DAX [32]，该系统允许通过内存映射直接访问物理NVMM设备。在LSNVMM中，NVMM区域被组织成日志（3.3），一个地址映射机制将虚拟家庭空间地址转换为日志位置（3.2）。应用程序通过我们的库访问NVMM区域，该库使用地址映射机制插入对该区域的所有内存访问。

恢复：为了实现有效的地址转换，地址映射被存储在DRAM中。在正常的进程关闭时，我们会压缩内部dram地址映射和其他必要的元数据，并将它们刷新到NVMM，以便在进程重新启动时快速恢复它们。但是，如果发生系统崩溃，则DRAM数据将丢失。因此，我们必须重新构建dram内的数据结构。为了加快此过程，**恢复将使用线程级并行性来执行**（更多细节见4.5）。

### 地址变换/映射

使用我们的地址映射机制，应用程序与NVMM的交互方式与DRAM构建数据结构基本相同。它们不需要更改使用灵活的常规虚拟内存地址和指针的内存访问模型。但是，他们必须采用事务接口来对类似于现有PTM系统的数据结构进行原子更改。我们将应用程序视图中的地址称为**家庭地址**，将应用程序隐藏的日志位置称为**日志地址**。

我们使用树状结构来维护从家庭地址到日志地址的映射。**从逻辑上讲，树中的一个节点包含一对{主地址，长度}，表示主空间中的一个区域，以及该区域被映射到的日志地址**。使用树而不是哈希表的基本原理是，在基于平面地址空间的系统中，分配粒度与访问粒度并不完全相同。例如，应用程序可以使用pmalloc分配一个大型结构，但只在事务中读/写其中的一小部分。因此，我们需要对与分配对象不对齐的任意访问进行翻译支持。

地址映射的效率对我们的系统至关重要。传统的日志结构系统的延迟主要由数据访问的磁盘/SSD延迟决定。此外，这种数据访问的粒度很大（例如，512B的块大小），并且频率较低。然而，在我们的例子中，NVMM更快，在几个字节的粒度中访问更频繁。因此，它需要仔细地设计地址映射。在树上的一个操作的时间复杂度为O（log n）。我们使用了几种优化方法来降低这种操作的实际成本。图3描述了支持这些优化的主要数据结构。

**两层映射**。树操作的平均成本与树的高度成正比，因此我们的第一个优化目标是大幅降低树高。这个如果一棵大树被分割成许多小的树，就可以实现。我们通过拥有两层地址映射来实现这一点。在第一层中，我们将主空间划分为固定长度的分区，这样一个主地址就可以简单地除以分区长度，成本低至一个CPU周期，以确定地址位于哪个分区。在第二层中，每个分区都有一个小树，用于进一步的地址查找（图3）。**我们的方法可以多次降低树高。对于真实世界的工作负载，这种优化平均提高了事务吞吐量39.6%**（5.2）。

**组更新**。机会主义地合并树节点是进一步减少节点数量从而减少树的高度的另一种方法。当两个兄弟节点包含连续的主地址并映射到连续的日志地址时，可以合并。事务中的空间本地写操作可以利用这种优化。在每个NVMM事务中，我们首先在DRAM中缓冲所有写入，并在事务提交时将这些与连续的主地址合并。一组组合写入被附加到日志中，地址映射树以最少的次数更新地址。根据我们的评估（5.2），该优化实现了42.3%的交易吞吐量提高。

**跳过列表和锁**。我们选择跳过列表[42]，一个平衡树的概率替代方案，作为我们的树数据结构（图3）。我们选择的主要原因是，虽然平均支持O（log n）操作，但跳过列表不像b树那样严格平衡的树那样需要复杂的再平衡操作。

由于大量读写而引起的锁争用可能会降低此类系统的吞吐量。相反，**通过利用跳过列表，我们消除了对只读操作的锁定。特别是，跳过列表的更新只涉及对单个链表的简单指针操作**。利用CPU的原子字写（为x86对齐64位），这种更新是以一种从原子到无锁的只读操作的方式实现的。通过避免这种锁争用，我们可以看到在我们的实验中（5.2）中有4个线程的事务吞吐量提高了48.9%。

**树节点缓存**: 我们为每个工作线程配备了一个**线程本地缓存**，该缓存存储了最近访问的主地址和指向树中节点的指针（图3）。当程序访问一个地址时，我们的库首先搜索高速缓存。如果被击中，库直接获得指向包含请求地址映射的树节点的指针；否则，需要进行完整的树查找，生成的节点将添加到缓存中。这种缓存机制是有效的，**因为内存访问之间具有固有的时间和空间局部性。正如我们的实验所显示的，一些记忆区域是热的和频繁访问的，而记忆访问倾向于聚集在64 B个区域内**。命中率平均为92.2%，而引入树节点缓存会导致事务吞吐量平均增加30.1%（5.2）。

我们调整了一个常规的哈希表设计，以满足树节点缓存的特殊要求。也就是说，一旦一个节点被缓存，其映射区域内的地址往往是一个缓存命中。普通散列表不给出这样的特性，因为缓存的地址是随机分布的。例如，缓存一个从0x1000开始的64 B区域的节点。如果对地址0x1008的访问属于另一个桶中，它将失去检查这个节点的机会，因此会错过。为了解决这个问题，**我们故意通过使用集结合性来增加一定的碰撞。基于上面的观察，我们尝试将64 B范围内的地址路由到同一个桶**，这样就可以用可能覆盖它们的**链式**树节点来检查附近的地址。为了实现这一点，我们选择一个地址的高阶位作为它的哈希值。因此，顺序地址很有可能落入一个桶中。

### NVMM组织

我们的NVMM组织的目标是允许每个线程以最小的开销分配NVMM。为此，NVMM区域在物理上被组织成静态块，我们在其上面构建逻辑日志。多个数据块可以链接到一个列表中。**我们选择一个相对较小的块大小（例如，32 KB）**，因为典型的NVMM写块数很小；此外，一个单独的块用小尺寸可以更快地逐步清洁和回收。

块有助于减少多个线程之间的争用。我们维护一个自由块的全局池，每个线程都有自己的正在使用的块列表。**当线程从全局池请求一些空闲块时，或从本地日志清理中获得这些块后，允许它缓冲一些空闲块**。这可以避免对全局池及其锁争用的频繁操作。

### 日志结构

NVMM区域中的日志由一组数据块组成。在我们的系统中有多个日志共存。它不同于传统的基于磁盘的日志结构系统，即每个磁盘往往只有一个日志，因为磁盘只有一个磁盘头，并且顺序访问是第一优先级。使用快速随机访问，NVMM保证了不同的设计，通过使用线程本地日志来支持线程级并行。此外，每个线程都有多个日志以提高日志清理效率，我们将在本节中描述。LSNVMM使用了一些日志清理器来收集以块的形式积累的空闲空间。空闲空间来自于pfree操作或已更新的旧数据。**我们使用一个后台线程来运行一个清理器**。

**日志条目**。一个日志条目包含两种元数据。首先，**针对已修改或已分配的内存区域的映射**。当日志清理器扫描块时，它通过从地址映射树中查找主地址来检查每个日志项的活性。第二，是每个自由区域的墓碑。在事务中从不访问墓碑，但在恢复路径上用于过滤出已释放的区域。在顶部的日志条目中，我们构建事务。事务由它通过内存存储和（删除）分配生成的所有日志条目组成。

**清洁政策**。日志清理器以一种紧凑的方式将稀疏的实时数据从几个块移动到一个新的块中，并回收清理后的块。选择实时数据量低于一个阈值（在我们的设置中默认情况下为20%）的块来进行清理。

我们为日志清理设计了三种优化方法。(1)**快速清理**：当块中的所有日志条目都过期时，可以安全地回收块。这可以快速完成，因为我们只需要修改几个列表指针就可以将块移动到空闲块列表，而不需要数据复制。(2)**单独的日志**：我们观察到，内存存储总是比内存分配具有更好的局部性。这意味着将它们混合在一个日志中可能会增加日志清洗成本，并减少快速清洗的机会。因此，**我们为每个线程设计了单独的日志，服务内存存储的更新日志，服务内存分配的分配日志和只存储墓碑的交易位置日志**。(3)**并行清洗**：为了有足够的日志清洗吞吐量，我们对不同的块使用多个后台线程执行日志清理。

## 实现

### Home空间管理

内存分配和访问是家庭空间管理的两个主要功能。我们利用现有的事务性内存系统的实现来实现这些功能。但是我们向事务内存添加了持久性： (1)必要的分配元数据存储在NVMM中，以便在崩溃后可以重新构建家庭地址空间，(2)已提交的事务存储在NVMM中，以便数据更新是持久的。接下来，我们将详细介绍底层的机制。

**家庭空间分配（虚拟地址空间分配）**。考虑到64位的家庭地址空间是虚拟的，并且足够大，碎片化并不是一个严重的问题。因此，我们选择当前的内存分配器Hoard [6]和dlmalloc [26]来实现家庭空间分配。Hoard服务的内存分配小于8 KB，而dlmalloc处理较大的[50]。

两个分配器的状态在崩溃时始终使用数据存储的元数据进行重建，因此，无需花费运行时间来确保状态的持久性。以Hoard为例。它将主空间组织成超级块，每个超级块服务于一定大小的分配请求（例如，一个8 KB的超级块包含一个16 B分配的数组）。超级块的元数据（位置和分配大小）存储在NVMM中。有了这些信息，我们只需依靠日志来推断崩溃后的分配状态。因此，家庭空间分配不会产生任何持久的操作。

**事务性内存**。应用程序对主空间数据的访问受到事务的保护。Intel STM编译器[1]用于检测常规的C/C++代码和事务注释。程序员放置关键字__tm_atomic和一对大括号来指定事务的范围。当事务开始时，发出内存加载、存储和提交时，编译器会自动生成进入我们的事务系统的调用。每个事务都包含一个临时的私有写集，其中包含所有已写值及其地址，这些值对并发事务是不可见的。当事务分配内存时，系统会在主空间中快速分配所请求的大小，并返回其主地址。在此之后，所有对新分配空间的写入都在**易失性写集中被缓冲**。

在提交事务时，已分配的内存、写入旧数据的操作都将持久化到日志中。同样，还记录交易，以确保内存不会泄漏。TinySTM事务可以同时接收对易失性区域和NVMM的内存写入。LSNVMM库负责过滤掉对易失性内存的写入，并在提交事务时以崩溃一致的方式将这些写入持久保存到NVMM中。执行组更新优化，以合并具有连续主地址的NVMM写操作。然后，为每个NVMM写入生成一个日志条目，并刷新到NVMM中的日志。然后每个NVMM写入都获得它的日志地址，并且库将从主地址到日志地址的映射插入到全局地址映射树中。

### 日志空间管理

从上而下，日志存储的层次结构如下：(1)日志存储在多个固定长度的块中；(2)在一个或多个块中，构成日志的事务存储在事务块中；(3)在一个事务块中，事务的内存分配和更新存储在日志条目中。我们现在用一个自下而上的顺序来描述这些组件。

**日志条目**。每个日志条目都有一个标头和数据。报头包括(1)一个47位的主地址来记录数据的起始主地址，(2)一位来表示条目是否是一个墓碑，(3)一个16位的大小来记录数据长度。47位足以保存一个主地址，因为我们记录了在NVMM区域中该地址的偏移量。头后面是日志数据。此条目结构同时用于更新日志和分配日志。

事务块。属于一个事务的一组日志条目构成了一个事务块的有效负载。前导符包含以下字段： (1)一个64位的版本号，用于记录事务的提交时间。在我们的实现中，它是由TinySTM为每个事务生成的单调递增的、全局唯一的时间戳。(2)一个48位的peer指针，它指向另一个事务块（例如，在当前块被填满时的不同块中），如果当前日志是更新日志，则是分配日志中，反之亦然。因此，**一个事务的所有块都形成了循环单链列表**。(3)用于记录当前事务块中的日志条目的数量。如果该数字不足以计算事务的所有项，则可以将更多的块链接到当前块。(4)一个使用CRC32错误检测代码的32位校验和，它是针对整个事务块计算出来的。

由于**逻辑事务可能包含跨更新和分配日志的多个事务块**，因此这些块之间的一致性成为一个问题。我们必须在两种情况下处理这个问题。第一种情况是，当崩溃中断了事务提交时。LSNVMM可以通过在恢复时检查每个事务块的校验和来检测这种情况，如果它的任何一个事务块无效或丢失，则丢弃该事务。第二种情况是，由于日志清理，事务块被移动到另一个块。因此，引用已移动块的对等指针将不再有效。**但是，只要包含的事务被安全地提交，这种不一致性就不会带来任何问题，因为对等指针只用于检测未提交的事务**。因此，我们只需要从包含未提交事务的日志尾部进行日志清理。

**块**。块的有效负载是构成日志一部分的事务块序列。块由它们的头进行双重连接。此外，报头还包含一个标志来表示该块是属于更新日志还是属于分配日志。如果事务块包含的日志条目大于块的剩余空间，**则该条目可以被分割为更多的空间，并存储在其他块中的链接对等块中**。

### Skip List

一个地址映射树被实现为一个并发的跳过列表。通过使用插入为例，我们展示了跳过列表如何以并发的方式操作。在跳过列表中，插入节点涉及将节点插入到多个级别。对于每个级别，插入与单链表的插入相同，它可以通过原子指针更新的壮举来原子地实现。我们从底部向上开始进行插入。一旦节点插入到底层，插入是有效的。插入到上层只会影响查找性能。因此，插入对并发读取在逻辑上是原子性的。

虽然读取是无锁的，但任何树的结构更新（例如，插入或删除）**都必须持有一个锁来控制整个树，因为并发的更新可能会相互破坏**。但是由于我们的设计中有大量这样的树，我们仍然可以保持高的更新并发性。

树节点缓存还需要仔细的并发控制。**我们必须检查命中的节点是否仍然持有所请求的主地址，因为该节点可能已经被删除和回收**。因此，我们两次检查节点的家乡地址——在读取节点的日志地址之前和之后。 如果两个检查都匹配，则日志地址必须有效。(两次读)

### 日志清理

当内存利用率超过一个阈值时，一些后台清理线程就会开始与事务线程并行工作。清理步骤如下： (1)根据3.3中的策略，识别出一组受害者块。对于每个受害者块，**对其所有日志条目进行扫描，通过检查地址映射树vt中的最新版本来确定每个条目中数据的活性**。如果vt高于当前事务处理版本，则将丢弃该条目。(2)对于保留活动条目的事务块，将重新计算前导（entry编号和校验和），并将整个块附加到新块。(3)对于移动的事务块，将运行准TinySTM事务，以使用活动条目的新日志地址更新全局映射。准事务只是为了强制执行并发性控制。(4)在将所有事务块移出受害者块之后，将通过将该块添加到全局空闲块池来回收该块。

> 扫描的方式，是否过于慢

### Recovery

我们的恢复工作分为两个阶段，以类似于map-reduce的方式最大化线程并行性。在第一阶段中，我们将所有日志块分派给恢复线程以进行并行处理。每个线程的主要任务是扫描分配的块，并通过其主地址的分区将有效的日志条目分组。在此阶段之后，每个线程都包含一个由主分区索引的数组，并且数组中的每个元素都有一个属于该分区的日志条目列表。请注意，这个临时日志条目结构只包含指向NVMM中数据的指针和必要的元数据（版本号）。

在第二个阶段中，每个恢复线程负责不同的主分区，其任务是重放属于这些分区的日志条目。为此，上述日志条目列表在线程之间被打乱，以便每个线程都包含由线程负责的分区列表。然后，对于每个分区，负责的单个线程根据其主地址和版本号对该分区的所有日志条目进行排序，然后选择具有最新版本的条目，并将它们的地址映射插入到该分区的全局地址映射树中。该方法类似于映射减少，避免了大多数线程争用。
