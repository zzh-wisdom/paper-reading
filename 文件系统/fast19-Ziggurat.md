# fast19-Ziggurat

## 摘要

与传统磁盘相比，新兴的快速的、字节可寻址的非易失性主存（NVMM）提供了存储性能的巨大提高。我们介绍了Ziggurat，一个结合了NVMM和慢速磁盘的分层文件系统，它**创建了一个具有接近NVMM的性能和大容量的存储系统**。Ziggurat根据应用程序**访问模式、写大小和应用程序阻塞到写完成**的可能性，引导传入的写到NVMM、DRAM或磁盘。Ziggurat配置应用程序的访问流，以预测个人写入的行为。在后台，Ziggurat估计了文件数据的“温度”，并将冷文件数据从NVMM迁移到磁盘上。为了充分利用磁盘带宽，Ziggurat将数据块合并成大型的顺序写入数据。实验结果表明，与单独在SSD上运行的EXT4和XFS相比，使用少量NVMM和大型SSD的Ziggurat分别实现了高达38.9×和46.5×的吞吐量提高。随着NVMM数量的增加，边缘拉的性能会提高，直到它与仅NVMM文件系统的性能相匹配。

目标：接近NVMM的性能和大容量的存储系统

考虑负载，将数据分别导到DRAM，NVM，DISK

1. 访问模式
2. 写大小
3. 应用等待写完成的可能性（异步或同步）

工作点:

1. 预测写入模式（是否阻塞）
2. 估计文件数据温度，冷数据搬迁到DISK
3. 合并大块写入到DISK

## 引言

新兴的快速、可字节寻址的持久内存，如电池支持的NVDIMMs [25]和3D-XPoint [24]，有望显著提高存储系统的性能。与传统的基于块的存储设备相比，这些非易失性存储器技术提供了更高的吞吐量和更低的延迟。

在NVMM上。这些文件系统利用持久性内存的直接访问（DAX）特性来绕过页面缓存层，并为用户应用程序提供对文件数据的直接访问。

持久内存的高性能需要付出高昂的代价。持久内存每字节的平均价格高于 SSD，而 SSD 和硬盘驱动器的容量可扩展到比 NVMM 大得多的容量。 因此，**对成本敏感或需要比 NVMM 提供的更大容量的工作负载将受益于可以利用两种技术优势的存储系统：NVMM 的速度和磁盘的容量**。

**分层**是解决这个困境的一个办法。分层文件系统管理异构存储设备的层次结构，并将数据放置在存储设备中，这非常符合数据的性能要求和应用程序未来的访问模式。

NVMM支持小的（例如，8字节）写，并提供类似DRAM的延迟，以及DRAM的写延迟。**这使得将数据和元数据放在哪里的决定变得更加复杂**：系统必须决定最初将写数据（DRAM或NVMM）放在哪里，如何将元数据、新写入的数据和应用程序可能读取的数据之间划分NVMM。

**第一个挑战**是如何充分利用NVMM的高带宽和低延迟。使用NVMM引入了一种比基于磁盘的存储系统更有效的方法来保存数据。文件系统可以通过将同步写入写入NVMM来持久化同步写入，这不仅可以绕过页面缓存层，还可以从关键路径中删除磁盘访问的高延迟。然而，DRAM页面缓存仍然比NVMM具有更高的吞吐量和更低的延迟，这使得它在对磁盘层执行异步写入方面具有竞争力。

**第二个挑战**是如何协调 NVMM 的随机访问性能与磁盘和 SSD 支持的顺序访问。 在具有 NVMM 和磁盘的分层文件系统中，带宽和延迟不再是不同存储层之间的唯一区别。 与磁盘相比，NVMM 的顺序性能和随机性能之间的差距要小得多，这使得它能够吸收随机写入。 同时，**文件系统应该利用 NVMM 来最大化磁盘写入和读取的顺序性**。

我们建议使用Ziggurat，一个跨越NVMM和磁盘的分层文件系统。Ziggurat在文件写入和数据迁移过程中通过智能数据放置来利用NVMM的好处。Ziggurat包括两个放置预测器，它们分析文件写入序列，并预测传入的写操作是否大且稳定，以及对文件的更新是否可能同步。然后，Ziggurat根据预测将传入的写入引导到最合适的层：**将同步更新的文件转到NVMM层，以最小化同步开销**。**小的随机写入也会转到NVMM层，以完全避免对磁盘的随机写入。对异步更新文件的大的顺序写入将进入磁盘**。

我们在NVMM中实现了一个有效的**迁移机制**，以便在NVMM中为传入的文件写入腾出空间，并加速读取到频繁访问的数据。我们首先配置文件数据的温度，并选择最冷的文件数据块进行迁移。在迁移过程中，Ziggurat会合并相邻的数据块，并将它们大块地迁移到磁盘上。Ziggurat还可以根据应用程序的访问模式来调整迁移策略。

本文的贡献包括：

1. 我们描述了一个**同步预测器**，以有效地预测应用程序是否可能阻塞以等待完成的写入。
2. 我们描述了一个**写大小预测器**来预测对文件的写入是否既大又稳定。
3. 我们描述了一种利用不同存储设备的特性来执行**高效迁移的迁移机制**。
4. 我们设计了一个**自适应的迁移策略**，它可以适应用户应用程序的不同访问模式。
5. 我们实施并评估了Ziggurat，以证明预测因子和迁移机制的有效性。

我们使用一组微观和宏观基准来评估齐古拉。我们发现，即使在小NVMM下，Ziggurat也能够在许多工作负载上获得接近NVMM的性能。与单独运行SSD的EXT4和XFS相比，Ziggurat使用少量的NVMM和一个大型的SSD，分别实现了高达38.9×和46.5×的吞吐量提高。随着NVMM数量的增加，Ziggurat的性能会提高，直到它几乎与仅NVMM文件系统的性能匹配。

## 背景

Ziggurat主要针对新兴的非易失性存储器技术和传统的基于块的存储设备（如ssd或hdd）。本节提供了NVMM和磁盘的背景知识，以及NVMM所基于的NOVA文件系统。

### Storage Technologies

新兴的非易失性主存（NVMM）、固态驱动器（SSD）和硬盘驱动器（HDD）技术具有其独特的延迟、带宽、容量和特点。表1显示了不同存储设备的性能比较。

非易失性存储器通过CPU的内存控制器提供字节寻址性、持久性和直接访问。电池支持的NVDIMMs [25,26]已经有一段时间了。无电池非易失性存储器技术包括相变存储器（PCM）[22,28]、忆阻器[31,33]和自旋扭矩传输RAM（STTRAM）[7,20]。英特尔和美光的3D-XPoint [24]将很快上市。所有这些技术都提供了比DRAM更长的延迟和更高的密度。3D-XPoint也出现在Optane ssd[17]中，使ssd比基于闪存的ssd要快得多。

### The NOVA File System

Ziggurat是基于NOVA [32]实现的，这是一个NVMM文件系统，旨在最大化混合内存系统的性能，同时提供强大的一致性保证。下面，我们将讨论与Ziggurat最相关的NOVA设计中的文件结构和可伸缩性方面。

NOVA为每个inode维护一个单独的日志。NOVA还在DRAM中维护基数树，该索引将文件偏移量映射到NVMM位置。inode、它的日志和它的数据页面之间的关系如图2a所示。对于文件写入，NOVA会在inode日志中创建写入条目（用于数据更新的日志条目）。每个写条目都包含一个指向新写数据页面的指针，以及其修改时间（mtime）。在NOVA创建一个写条目后，它会更新NVMM中inode日志的尾部，以及dram中的radix树。

NOVA使用每个cpu的分配器来进行NVMM空间和percpu日志来管理复杂的元数据更新。这支持了并行块分配，并避免了日志记录中的争用。此外，NOVA有每个cpu的inode表，以确保良好的可伸缩性。

## Ziggurat Design Overview

Ziggurat是一个跨越NVMM和磁盘（硬态或固态）的分层文件系统。我们的设计是为了充分利用NVMM和磁盘的优势，并为广泛的访问模式提供较高的文件性能。

三个设计原则推动了我们在设计拼图架时所做出的决定。首先，Ziggurat应该是快速第一。它应该使用磁盘来扩展NVMM的容量，而不是像[14,15]以前一些系统的那样使用NVMM来提高磁盘的性能。其次，Ziggurat努力通过放置和移动数据来节约，以避免浪费稀缺的资源（例如，NVMM容量或磁盘带宽）。第三，Ziggurat应该通过动态学习给定工作负载的访问模式，并调整其数据放置决策以进行匹配来进行预测。

这些原则影响了Ziggurat设计的各个方面。例如，快速优先意味着，在常见情况下，文件写入会转到NVMM。但是，如果Ziggurat预测在NVMM中引导特定的写不会帮助应用程序的性能（例如，如果写很大且异步），那么它将会成为例外。

或者，如果写是小的和同步的（例如，到日志文件），Ziggurat将将它们发送到NVMM，检测日志项何时“冷却”，然后将这些小的写入聚合到更大的、对磁盘的顺序写入。

Ziggurat使用两种机制来实现这些设计原则。第一种是由一对预测器驱动的放置策略，它度量和分析过去的文件访问行为，以预测未来的行为。第二种是有效的迁移机制，它在层之间移动数据，以优化NVMM性能和磁盘带宽。迁移系统依赖于一种简单但有效的机制来识别从NVMM移动到磁盘的冷数据。

我们在一个由NVMM和SSD组成的简单两层系统的背景下描述Ziggurat，但它可以使用任何块设备作为“较低”层。Ziggurat 还可以通过跨不同层迁移数据块来处理多个块设备层。

### Design Decisions

为了实现我们的目标，我们在齐古拉特做出了以下设计决定。

- **Send writes to the most suitable tier**。虽然NVMM是NVMM中最快的层，但文件写不应该总是转到NVMM。NVMM最适合于小的更新（因为对磁盘的小写操作速度很慢）和同步写操作（因为NVMM具有更高的带宽和更低的延迟）。但是，对于较大的异步写入，目标磁盘更快，因为Ziggurat可以缓冲DRAM中的写入数据，而且对磁盘的写入可以在后台进行。Ziggurat使用它的同步性预测器来分析对每个文件的写序列，并预测未来的访问是否可能是同步的（即，应用程序是否会在不久的将来调用fsync）。
- **Only migrate cold data in cold files**。在迁移过程中，Ziggurat的目标是冷文件的冷部分。不均匀访问的文件中的热文件和热数据仍然在较快的层中。当快速层的使用超过一个阈值时，Ziggurat会选择具有最早的平均修改时间的文件来进行迁移（第5.1节）。在每个文件中，Ziggurat会迁移比平均水平更老的块。除非整个文件是冷的（也就是说，它的修改时间不是最近的），在这种情况下，我们会迁移整个文件。
- **High NVMM space utilization**。Ziggurat充分利用NVMM空间来提高性能。Ziggurat使用NVMM来吸收同步写入。Ziggurat基于应用程序的读写模式为NVMM使用了一个动态迁移阈值，因此它可以充分利用NVMM来有效地处理文件的读写。我们还实现了反向迁移（第5.2节），以便在运行以读取为主导的工作负载时将数据从磁盘迁移到NVMM。
- **Migrate file data in groups**。为了最大化磁盘的写带宽，Ziggurat尽可能按顺序地执行到磁盘的迁移。放置策略确保大多数小的随机写入进入NVMM。但是，将这些小的写入项直接迁移到磁盘将会受到磁盘的随机访问性能较差的影响。为了提高迁移效率，Ziggurat将相邻的文件数据合并成大的块进行迁移，以利用连续的磁盘带宽（第5.3节）。
- **High scalability**。Ziggurat扩展了NOVA的每个cpu存储空间分配器，以包括所有的存储层。**它还使用每个cpu的迁移和页面缓存回写线程来提高可伸缩性**。

### File Operations

图1说明了Ziggurat如何处理跨多个层的文件（写、同步、附加、迁移、读取和mmap）上的操作。

- Write。应用程序在(a).中的三个顺序写入初始化文件的前24个块Ziggurat首先检查来自同步性预测器和写大小预测器（第4节）的结果，以决定哪一层应该接收新的数据。在本例中，这三个写操作很大，Ziggurat预测访问是异步的，因此Ziggurat将这些写入引导到磁盘。它将数据写入DRAM中的页面缓存，然后异步地写入磁盘。
- Synchronize。该应用程序在(b).中调用fsync Ziggurat遍历文件的写日志条目，并重新写入DRAM页面缓存中的脏数据页。回写线程合并所有相邻的脏数据页，以执行对磁盘的大顺序写入。如果文件数据在NVMM中，那么fsync将是无操作的。
- Append。在fsync之后，应用程序执行8个同步写入，以在(c).中的文件末尾添加8个块。放置预测器识别小同步写的模式，Ziggurat将写引导到NVMM。
- Migrate。当文件在(d)中变得变冷时，Ziggurat将驱逐DRAM的前24个数据页，并使用组迁移将最后8个数据块从NVMM迁移到磁盘（第5.3节）。
- Read。 用户应用程序读取(e)中的最后8个数据块。Ziggurat将它们从磁盘获取到DRAM页面缓存。
- Memory map。用户应用程序最终向(f)中的文件头发出一个mmap请求。齐古拉特使用反向迁移将数据带到NVMM中，然后将页面映射到应用程序的地址空间中。

## Placement Policy

Ziggurat引导同步的或小的写入到NVMM，但它引导异步的、大的写入到磁盘，因为写入DRAM页面缓存比写入NVMM要快，而且Ziggurat可以在后台写入磁盘。

它使用两个预测器来区分这两种类型的写入。

- **同步预测器**：同步性预测器可以预测应用程序是否有可能在不久的将来调用该文件上的fsync。同步性预测器计算在两次对fsync的调用之间写入该文件的数据块的数量。如果这个数字小于一个阈值（例如，在我们的实验中是1024），预测器将其归类为一个同步更新的文件。预测器还将对使用O SYNC打开的文件的写入视为同步。
- **写入大小预测器**：写大小预测器不仅确保写入足够大，可以有效地利用磁盘带宽，而且确保相同地址范围内的未来写入也可能很大。第二个条件很关键。例如，如果应用程序使用大的I/O初始化一个文件，然后执行许多小的I/O，这些小的新写条目将读取并失效离散块，增加碎片化，并导致许多随机磁盘访问以服务于未来的读取。

Ziggurat的写大小预测器在每个写条目中保留一个计数器，以指示写大小是否又大又稳定。当Ziggurat重写一个旧的写条目时，它会首先检查写的大小是否足以覆盖原始日志条目所占用的至少一半的区域。如果是这样，Ziggurat将旧写条目的计数器值转移到新的，并将其增加1。否则，它将将计数器重置为零。如果这个数字大于4（一个可调参数），Ziggurat将这个写归类为“大”。大且异步的写入将转到磁盘。

## Migration Mechanism

迁移的目的是在NVMM中为传入的文件写入腾出空间，并加快对频繁访问的数据的读取。我们使用基本迁移将数据从磁盘迁移到NVMM，以便在运行读取主导的工作负载时充分利用NVMM空间。我们使用组迁移，通过合并相邻的数据块，将数据从NVMM迁移到磁盘，以实现高迁移效率，并为未来的写入腾出空间。只要迁移机制足够有效，Ziggurat对于大多数访问都可以实现接近NVMM的性能。

在本节中，我们首先描述Ziggurat如何识别好的迁移目标。然后，我们将说明它如何有效地迁移数据，以最大化磁盘的带宽（基本迁移和组迁移。最后，我们将展示如何有效地迁移文件日志）。

### Migration Profiler

Ziggurat使用一个迁移分析器来识别要从NVMM迁移到磁盘的冷数据。

**Implementation**。Ziggurat首先识别要迁移的冷文件。Ziggurat通过维护冷链表（cold lists）来描述每个文件的温度，每个存储层上的每个cpu文件链表，根据文件中所有块计算的平均修改时间（amtime）进行排序。每个cpu的冷列表对应于将文件从一层迁移到另一层的每个cpu迁移线程。Ziggurat在修改文件时更新冷列表。为了识别冷文件中最冷的块，Ziggurat跟踪文件中每个块的mtime。

要迁移数据，Ziggurat将从冷列表中弹出最冷的文件。如果弹出的文件的mtime不是最近的（超过30秒前），那么Ziggurat将整个文件视为冷文件，并迁移所有文件。否则，文件块的修改时间是分散的，然后Ziggurat迁移时间早于文件amtime的写条目（log entries）。因此，文件的冷部分将被迁移到较低的层中，而文件的热部分将保留在原始层中。

**决定何时迁移**。大多数现有的分层存储系统（如[6,21]）使用固定的利用率阈值来决定何时将数据迁移到较低的层。但是，较高的阈值不用于写主导的工作负载，因为持久内存中的空间将被密集的文件写入所吞噬。在这种情况下，文件写入必须在迁移线程在NVMM中清理足够的空间之前停止，或者写入磁盘。

另一方面，较低的阈值是不适合读取主导的负载。因为读取必须从磁盘上加载更多的块，而不是从NVMM上。**我们基于文件系统的总体读写比率，实现了NVMM的动态阈值。随着系统的读写比的变化，迁移阈值从50%上升到90%**。

### Basic Migration

基本迁移的目标是将Ziggurat中最冷的数据迁移到磁盘。当上层的使用高于阈值时，每个cpu迁移线程将冷文件中最冷的数据迁移到磁盘。迁移过程重复，直到上层的使用再次低于阈值。

迁移的粒度是一个写条目。在迁移过程中，我们遍历in-DRAM radix树来定位文件中每个有效的写条目，migrate the write entries with mtime earlier than the amtime of the file.

图2a说明了Ziggurat如何将写入条目从NVMM迁移到磁盘的基本过程。第一步是在磁盘上分配连续的空间来保存已迁移的数据。边缘存储器将数据从NVMM复制到磁盘。然后，它向inode日志添加一个新的写入条目和迁移的数据块的新位置。之后，它将更新NVMM中的日志尾部和DRAM中的radix树。最后，齐古拉释放了NVMM的旧方块。

为了提高可伸缩性，**Ziggurat在写条目的粒度中使用锁**，而不是整个文件。Ziggurat在迁移期间锁定写条目，但文件的其他部分仍然可供读取。迁移不会阻止文件写入操作。如果任何前台文件I/O请求试图获取inode锁定，迁移线程将停止迁移当前文件，并释放锁。

如果在 DRAM 页面缓存使用率较低（即低于 50%）时将写入条目迁移到磁盘，则 Ziggurat 将在 DRAM 页面缓存中将页面做副本拷贝，以加速未来的读取。写块也将从中受益，因为未对齐的写块必须从它们的邻居写条目中读取部分块来填充数据块。（NVM实现部分写，则不需要读了）

Ziggurat实现了反向迁移，即使用基本迁移将文件数据从磁盘迁移到NVMM。由于NVMM可以有效地处理顺序和随机写，因此写条目被连续迁移而无需分组。文件mmap使用反向迁移来启用对持久性数据的直接访问。**当NVMM使用量较低时，反向迁移还可以优化以读取主导的工作负载的性能，因为其性能取决于内存的大小。**如果Ziggurat只能将数据从较快的层迁移到较慢的层，那么在运行以读取为主的工作负载时，NVMM宝贵的可用空间将保持空闲。与此同时，磁盘上的数据正在争夺一个有限的DRAM。反向迁移在这种场景中充分利用了NVMM。

### Group Migration

**组迁移避免了细粒度迁移，以提高传输效率，并最大化到磁盘的顺序带宽**。由于其数据放置策略，边缘器倾向于用小写来填充NVMM。通过基本迁移将它们从NVMM迁移到磁盘是效率低下的，因为这会导致磁盘的高随机访问延迟。

组迁移将NVMM中的小写条目合并为磁盘中的大顺序条目。它有四个好处： (1)它将小的随机写入合并为大的顺序写入，从而提高了迁移效率。(2)**如果再次读取迁移的数据，加载连续块比加载磁盘周围的分散块要快得多**。(3)通过合并写条目，日志本身会变得更小，从而减少了元数据访问开销。(4)它通过模拟垃圾收集来缓和由日志结构化写入引起的磁盘碎片化。

如图2b所示，组迁移的步骤类似于迁移写条目。在步骤1中，我们在较低层分配大量的数据块。在步骤2中，我们使用一个顺序写入将多个页面复制到较低的层。之后，我们添加日志条目，并更新inode日志尾部，从而提交组迁移。然后将释放陈旧的页面和日志。理想情况下，**组迁移大小（组迁移的粒度）应该设置为接近未来的I/O大小**，以便应用程序可以从磁盘连续读取来获取文件数据。此外，它不应该超过CPU缓存的大小，以最大限度地提高从磁盘加载写条目的性能。

### File Log Migration

当NVMM利用率过高时，Ziggurat除了迁移数据之外，还迁移文件日志，为热数据和元数据释放空间。齐古拉特定期扫描冷列表，并启动对冷文件的日志迁移。

边缘库将实时日志条目从NVMM复制到页面缓存中。在日志处理期间，日志条目被压缩成新的日志页。然后，它将新的日志页写入磁盘，并更新DRAM中的inode元数据缓存，以指向新的日志。之后，齐古拉原子地用新原木替换旧原木，并回收旧原木。

## 测试

由于持久性内存设备还不可用，我们使用DRAM上的NUMA效应来模拟NVMM的延迟和带宽。在我们的平台上有两个NUMA节点。在实验过程中，使用NUMA节点1的整个地址空间进行NVMM仿真。所有的应用程序都被固定为可以在NUMA节点0的处理器和内存上运行。表2显示了我们通过Intel内存延迟检查器[16]进行的实验平台的DRAM延迟。

我们将边缘器与不同类型的文件系统进行比较。对于基于NVMM的文件系统，我们将Ziggurat与NOVA [32]、Strata[21]（仅限NVMM）以及Linux上的基于DAX的文件系统进行了比较： EXT4-DAX和XFS-DAX。对于基于磁盘的文件系统，我们比较了Ziggurat和EXT4在数据日志模式（-DJ）下的情况，以及在元数据日志模式（-ML）下的XFS。EXT4-DJ和XFS-ML都提供了数据的原子性，类似Ziggurat。对于EXT4-DJ，日志保存在NVMM上的2 GB日志块设备（JBD）中。对于XFS-ML，元数据日志记录设备是2 GB的NVMM。我们将DRAM页面缓存的容量限制为10 GB。

对于分层文件系统，我们只对具有不同配置的Ziggurat进行比较。**据我们所知，Strata是目前唯一可用的跨越NVMM和磁盘的分层文件系统。然而，公开版本的Strata只支持少数应用程序，并且在运行数据集大小大于NVMM大小以及多线程应用程序的工作负载时遇到了困难**。

我们改变了齐古拉可用的NVMM容量，以显示不同的存储配置的性能变化。每个工作负载的数据集大小都小于64 GB。这种变化始于Ziggurat-2（即2 GB NVMM的边缘拉）。在这种情况下，大多数数据必须驻留在磁盘上，这迫使Ziggurat频繁地迁移数据以适应传入的写操作。Ziggurat-2对于EXT4-DJ和XFS-ML也是一个有趣的比较点，因为这些配置采用了不同的方法来使用少量的NVMM来提高文件系统性能。变异以Ziggurat-64（即含有64 GB的NVMM）结束。组迁移大小设置为16 MB。**我们运行每个工作负载三次，并报告这些运行中的平均值。**



## 更多参考

[6] CANO, I., AIYAR, S., ARORA, V., BHATTACHARYYA, M., CHAGANTI, A., CHEAH, C., CHUN, B. N., GUPTA, K., KHOT, V., AND
KRISHNAMURTHY, A. Curator: Self-managing storage for enterprise
clusters. In NSDI (2017), pp. 51–66.

KWON, Y., FINGLER, H., HUNT, T., PETER, S., WITCHEL, E., AND
ANDERSON, T. Strata: A cross media file system. In Proceedings of
the 26th Symposium on Operating Systems Principles (2017), ACM,
pp. 460–477.

[14] FANG, R., HSIAO, H.-I., HE, B., MOHAN, C., AND WANG, Y. High
performance database logging using storage class memory.
[15] HITZ, D., LAU, J., AND MALCOLM, M. A. File system design for
an nfs file server appliance. In USENIX winter (1994), vol. 94.
