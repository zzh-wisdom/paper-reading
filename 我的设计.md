# 我的设计

基于PM和SSD的混合数据页管理

正如论文所说，如果放弃原地修改，或者系统要保证data的崩溃一致性，就不能向用户提供虚拟地址接口，即用户不能再像使用内存那样（通过对地址的load/store）来使用NVM，而是使用系统封装好的特定API来读写，API内部处理一些地址转换、索引、崩溃一致性的工作。这样对于一个NVM数据，我们不用虚拟地址标志它，而是使用对象ID。

针对非对齐写入优化

> 是否考虑更大范围的数据块？不一定要按照页来管理

针对细粒度的冷热管理方案。

传统文件系统需要使用DRAM作为数据缓存，对于写入，需要缓存脏数据页，然后定期刷回存储介质中。而NVM具有掉电数据不丢失的特性，可以使用NVM作为写缓存，从而避免写回的高开销，进而在垃圾回收过程中，根据页的访问情况，将冷数据刷回SSD中。

4.1 Goals
We aim to support the following properties in the proposed file system with journaling on NVMe SSD.
(i) **Performance**. The overall I/O performance in terms of IOPS is significantly improved compared to
the original journaling file system.
~~(ii) **Stability**. The I/O performance is relatively stable over time.~~
(iii) **Durability**. Once a write transaction is committed to the journal successfully, it will survive
permanently.
(iv) **Low cost**. The additional resource consumption incurred by the new design is maintained at a low
level.

## 空间管理

OID分配：全局变量，原子增加。通过批量分配避免每次alloc对象都需要flush+fence

将空间切分成chunk（稍微小一点？4MB，再切成更小的Page来作为log比较好）
线程本地的chunk list（in use，free，to gc）

alloc size + free size，避免冲突

free的时候，需要把log entry标记为无效，为了避免随机写，添加free op log。后台进行GC，gc时会增加free size，当它发现该page的不在使用了，而且数据有效率低于一定的阈值，则可以进行page回收。page回收加入thread local的free list

> 暂时不考虑chunk丢回全局的操作

对于覆盖写的旧数据回收，通过扫描对象索引来发现，降低free的关键路径开销

优化1: 小对象的分配用单独的chunk，大对象用单独的chunk，即chunk有身份。因为不同的对象大小往往有不同的访问逻辑，相同大小的对象更有可能有相似的访问频率，这样可以提供cache的命中率

**需要重大创新的话**：或许可以考虑文件数据的extend方式索引，而不是按照page。考虑划分多级chunk、sub-chunk、page（或者说多级别page）

## 混合存储

可以在内存中维护一个LRU buffer，每次读页数据时，在内存中重构，这样后面的读就不用多次log跳跃索引，不过这样显然会增加NVM的写入流量，适当增加DRAM buffer的大小，可以吸收大部分更新

数据用ntstore，元数据用clwb

## 索引

MemC3 [41] redesigns the hash ta- ble with Cuckoo hashing and removes LRU chain pointers. However, it does not consider some operations such as cas for atomic updates, does not support TTL expiration or other advanced eviction algorithms.

> [41] Bin Fan, David G Andersen, and Michael Kamin- sky. Memc3: Compact and concurrent memcache with dumber caching and smarter hashing. In Presented as part of the 10th USENIX Symposium on Networked Sys- tems Design and Implementation (NSDI 13), pages 371– 384, 2013.

解决并发索引的问题。最简单的方式就是分片哈希，单个哈希一个锁。

另外可以额外增加一个thread local的哈希索引，减少哈希读写冲突。

最主要的问题是，写入时的哈希扩展问题。
TODO: 探索可以读写并发的哈希结构

inode的管理可以参考 TridentFS，固定最大的inode大小。用bitmap标志是否使用。

> 链表式哈希表

## 垃圾回收

垃圾回收通过全局扫面可能比较消耗带宽，考虑正常的访问中收集垃圾回收信息。全盘扫描肯定不行，但可以优化，
内存中维护GC链表，每线程一个

可以将垃圾回收和冷热管理相结合，冷的数据直接写回SSD。热数据考虑顺便进行碎片整理

选择算法：
(i) Greedy [30], which selects the sealed segments with the
highest GPs, and (ii) Cost-Benefit [30,31], which selects the
sealed segments that have the highest values GP∗age (where 1−GP
age refers to the elapsed time of a sealed segment since it
is sealed) for GC.

> 感觉Cost-Benefit简单实用

过多的GC线程会影响前台的IO性能，只有极端情况下，才开启多线程GC

Besides data placement, existing studies propose segment selection algorithms to reduce the WA of flash-level GC. In addition to Greedy and Cost-Benefit (§2.1), Cost-Age-Times [11] considers the cleaning cost, data age, and flash erasure counts in segment selection. Windowed Greedy [17], Random- Greedy [24], and d-choices [36] are variants of Greedy in segment selection. Desnoyers [14] models the WA of different segment selection algorithms and hot-cold data separation. SepBIT can work in conjunction with those algorithms.

## cache

尝试启用NVM作为二级Cache。DRAM中的cache淘汰时判断是否先写回NVM。

**思考可多线程扩展的LRU cache算法。**

调研NOVA是否在普通write的过程中就将数据写到NVM。如果是，我们只有在fsync的时候，将数据写回NVM也算是一个优化。

1. DRAM cache write。单独的每个page的元数据，标记哪些范围的数据是脏的（64B或者256B的粒度）。
2. 更简单的实现是，NVM中copy on write。原地修改，只有在fsync时才生成新版本。

具体来说，当 NVRAM 空间利用率超过预定义的阈值 TH（即高水位线）时，驱逐管理器开始执行驱逐，直到 NVRAM 空间利用率下降到另一个阈值 TL（即低水位线）以下 . 在当前实现中，TH 和 TL 分别设置为 100% 和 95%。

在选择驱逐数据（即元数据或文件数据）时考虑三个因素，即新近度、频率和大小。 TridentFS 尝试在 NVRAM 中保留最近使用、经常使用和小数据。 正如之前的研究[36, 37]所指出的，频率和新近度都是表明数据热度的重要因素。 另外，size也考虑，主要是因为驱逐大尺寸的数据有助于回收大量的NVRAM空间，可以用来容纳很多小尺寸的热点数据。

## 编程

loadable module

## 测试

<https://link.springer.com/article/10.1007/s10586-019-03023-y>

TridentFS（2014）这篇论文有对不同测试benchmark的负载分析，感觉小文件还挺多的，可以对小文件进行特殊的优化。

## TODO

寻找一个线程可扩展的LRU算法，缓存算法

## 注意

尝试线程本地缓存优化

写流量测试，flush的cacheline个数*大小（简单计算flush+fence的个数就好）
线程扩展性测试
NVM碎片测试

垃圾回收的影响

恢复时间
